{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFPMsIh+Om2t8n+fJJMj6p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" **Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization.**"],"metadata":{"id":"QwOtHZy_SRm7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8rHM5-KQlGj","executionInfo":{"status":"ok","timestamp":1741179682537,"user_tz":-330,"elapsed":2654,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"6f964eb4-542d-4e58-b465-9cf08d3ddf89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}],"source":["!pip install nltk\n"]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import (WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer)\n","from nltk.stem import PorterStemmer, SnowballStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n"],"metadata":{"id":"TbjD8V2OQmZS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download necessary NLTK data\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCwCcm1DQp8l","executionInfo":{"status":"ok","timestamp":1741179682662,"user_tz":-330,"elapsed":94,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"cf336ad1-3504-4bd2-f39f-eec780a6061b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["text =\"Hello there! This is the first assignment of NLP .\""],"metadata":{"id":"YGb5RQ--REdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Tokenization: Tokenization  is the process of breaking text into smaller units such as words or phrases.**"],"metadata":{"id":"pTR047LdS_Sn"}},{"cell_type":"markdown","source":["**Types -**"],"metadata":{"id":"zijnSYvcTJPW"}},{"cell_type":"markdown","source":["**1. Whitespace Tokenization: Splits text based on spaces.**"],"metadata":{"id":"I-zKPI8bTOrr"}},{"cell_type":"code","source":["# 1. Tokenization\n","print(\"\\n--- Tokenization ---\")\n","\n","# Whitespace-based Tokenization\n","whitespace_tokens = WhitespaceTokenizer().tokenize(text)\n","print(\"Whitespace Tokenization:\", whitespace_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcBiZpPCRQYX","executionInfo":{"status":"ok","timestamp":1741179682758,"user_tz":-330,"elapsed":30,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"a4af8488-1046-4092-b2cf-55593b3d8643"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Tokenization ---\n","Whitespace Tokenization: ['Hello', 'there!', 'This', 'is', 'the', 'first', 'assignment', 'of', 'NLP', '.']\n"]}]},{"cell_type":"markdown","source":["**2. Punctuation-Based Tokenization (WordPunctTokenizer): Splits text based on both spaces and punctuation.**"],"metadata":{"id":"GrJIrdeRToYo"}},{"cell_type":"code","source":["# Punctuation-based Tokenization\n","punctuation_tokens = WordPunctTokenizer().tokenize(text)\n","print(\"Punctuation Tokenization:\", punctuation_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5GsfOmZRQU4","executionInfo":{"status":"ok","timestamp":1741179682759,"user_tz":-330,"elapsed":16,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"e7976ac7-d9cc-4165-d9d9-1eec1186e1f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Punctuation Tokenization: ['Hello', 'there', '!', 'This', 'is', 'the', 'first', 'assignment', 'of', 'NLP', '.']\n"]}]},{"cell_type":"markdown","source":["**3. Treebank Tokenization: Uses the Penn Treebank rules to split text, handling contractions and punctuation properly.**"],"metadata":{"id":"zUJkvx_gTrEn"}},{"cell_type":"code","source":["# Treebank Tokenization\n","from nltk.tokenize import TreebankWordTokenizer # Ensure TreebankWordTokenizer is imported\n","treebank_tokens = TreebankWordTokenizer().tokenize(text) # Define treebank_tokens before using it\n","print(\"Treebank Tokenization:\", treebank_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e-3a8J-RP6Y","executionInfo":{"status":"ok","timestamp":1741179682776,"user_tz":-330,"elapsed":16,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"1c74ada4-c90e-4028-91d2-9aaa1e8e984e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Treebank Tokenization: ['Hello', 'there', '!', 'This', 'is', 'the', 'first', 'assignment', 'of', 'NLP', '.']\n"]}]},{"cell_type":"markdown","source":["**4. Tweet Tokenization: Special tokenization designed for social media text, preserving hashtags, emojis, and mentions.**"],"metadata":{"id":"AIBy1IH8TvLX"}},{"cell_type":"code","source":["# Tweet Tokenization\n","tweet_tokenizer = TweetTokenizer()\n","tweet_tokens = tweet_tokenizer.tokenize(text)\n","print(\"Tweet Tokenization:\", tweet_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeFHoWHIRvN8","executionInfo":{"status":"ok","timestamp":1741179682815,"user_tz":-330,"elapsed":19,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"4f2660fc-d442-4e23-90c6-904cfef82762"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tweet Tokenization: ['Hello', 'there', '!', 'This', 'is', 'the', 'first', 'assignment', 'of', 'NLP', '.']\n"]}]},{"cell_type":"markdown","source":["**5. Multi-Word Expression (MWE) Tokenization: Recognizes and keeps predefined multi-word expressions together.**\n","\n","\n","\n"],"metadata":{"id":"B65JCtuuTzco"}},{"cell_type":"code","source":["# Download the missing punkt_tab data package\n","import nltk\n","nltk.download('punkt_tab')\n","\n","# Multi-Word Expression Tokenization\n","mwe_tokenizer = MWETokenizer([(\"learning\", \"NLP\"), (\"including\", \"tokenization\")])\n","mwe_tokens = mwe_tokenizer.tokenize(nltk.word_tokenize(text))\n","print(\"MWE Tokenization:\", mwe_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXWMPVSRRvKp","executionInfo":{"status":"ok","timestamp":1741179682838,"user_tz":-330,"elapsed":20,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"3d8e9741-f355-4c02-b5c4-513bc8cb194b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MWE Tokenization: ['Hello', 'there', '!', 'This', 'is', 'the', 'first', 'assignment', 'of', 'NLP', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**2. Stemming : Stemming is the process of reducing words to their root form by removing suffixes. It does not guarantee real words but helps in reducing word variations.**"],"metadata":{"id":"MepPeHWTT2WY"}},{"cell_type":"markdown","source":["**1. Porter Stemmer: A simple and widely used stemming algorithm that removes common suffixes.**"],"metadata":{"id":"FKKW6xXrUVfH"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","input_string = \"running walking jumped easily cats\"\n","\n","# Tokenizing the input string\n","whitespace_tokens = word_tokenize(input_string)"],"metadata":{"id":"oK6J9AwsY4qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Stemming\n","print(\"\\n--- Stemming ---\")\n","\n","porter_stemmer = PorterStemmer()\n","snowball_stemmer = SnowballStemmer(\"english\")\n","\n","porter_stemmed = [porter_stemmer.stem(word) for word in whitespace_tokens]\n","print(\"Porter Stemmer:\", porter_stemmed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzymBhjyRvIv","executionInfo":{"status":"ok","timestamp":1741179713856,"user_tz":-330,"elapsed":31,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"654615e9-316b-4f19-da60-8b4b96c38424"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Stemming ---\n","Porter Stemmer: ['run', 'walk', 'jump', 'easili', 'cat']\n"]}]},{"cell_type":"markdown","source":["**2. Snowball Stemmer: An improved version of Porter Stemmer, supporting multiple languages and handling words better.**"],"metadata":{"id":"tvCkObOQUYri"}},{"cell_type":"code","source":["snowball_stemmed = [snowball_stemmer.stem(word) for word in whitespace_tokens]\n","print(\"Snowball Stemmer:\", snowball_stemmed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qeWWA8WkRvGt","executionInfo":{"status":"ok","timestamp":1741179713876,"user_tz":-330,"elapsed":18,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"b4f58bb0-582c-4b46-e25d-725ea0320339"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Snowball Stemmer: ['run', 'walk', 'jump', 'easili', 'cat']\n"]}]},{"cell_type":"markdown","source":["**Lemmatization:Lemmatization is a more advanced technique than stemming. Instead of chopping off suffixes, it converts words into their dictionary (base) form, called the lemma.**\n","\n","**WordNet Lemmatizer: Uses WordNet, a large lexical database, to find the base form of words. It considers the context and part of speech. bold text**"],"metadata":{"id":"igBmLaobUaBb"}},{"cell_type":"code","source":["# 3. Lemmatization\n","print(\"\\n--- Lemmatization ---\")\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(word, wordnet.VERB) for word in whitespace_tokens] # This line creates the 'lemmatized' variable and assigns it a value\n","print(\"Lemmatized:\", lemmatized) # Now you can print 'lemmatized' as it's been defined"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unQ8IV6CSWt3","executionInfo":{"status":"ok","timestamp":1741179718508,"user_tz":-330,"elapsed":4630,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"bf5efe0b-08c2-4bba-c4a0-a9dd060861a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Lemmatization ---\n","Lemmatized: ['run', 'walk', 'jump', 'easily', 'cat']\n"]}]}]}