{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLJrhgIxnA8UXJ3EMsQ4vB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Perform bag-of-words approach (count occurrence, normalized count ccurrence), TF-IDF on data. Create embeddings using Word2Vec**"],"metadata":{"id":"jiYk2UXbZzpi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0QA_6h4kW6m3","executionInfo":{"status":"ok","timestamp":1738813550771,"user_tz":-330,"elapsed":9646,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"d65c357a-2452-464c-c186-5fd702d917b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}],"source":["pip install nltk scikit-learn gensim\n"]},{"cell_type":"code","source":["import nltk\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJ-tfH9vXOkX","executionInfo":{"status":"ok","timestamp":1738813563832,"user_tz":-330,"elapsed":13037,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"a57f9069-4d02-47f4-da37-f6eab19f683d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["documents = [\n","    \"Natural Language Processing is amazing!\",\n","    \"Machine learning and deep learning are subfields of AI.\",\n","    \"Natural Language Processing (NLP) is part of AI and Machine Learning.\",\n","    \"Deep learning improves NLP tasks.\",\n","]"],"metadata":{"id":"4q9nMz9wXQhn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **1. Bag-of-Words (BoW)**\n","Counts word occurrences in each document.\n","Creates a document-term matrix.\n","Normalized count occurrence (TF) is automatically handled by TF-IDF.\n","\n","\n"],"metadata":{"id":"oPxNulMqX1Ul"}},{"cell_type":"code","source":["# 1. Bag of Words (Count occurrence & Normalized count)\n","print(\"\\n--- Bag-of-Words (BoW) ---\")\n","\n","# Convert text to count vectors\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(documents)\n","\n","# Convert to DataFrame\n","bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n","print(\"\\nBoW Matrix (Raw Count):\\n\", bow_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOngmH2MXU6O","executionInfo":{"status":"ok","timestamp":1738813580046,"user_tz":-330,"elapsed":67,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"d1b4798d-139c-48dd-8d44-c52bf6a57ae8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Bag-of-Words (BoW) ---\n","\n","BoW Matrix (Raw Count):\n","    ai  amazing  and  are  deep  improves  is  language  learning  machine  \\\n","0   0        1    0    0     0         0   1         1         0        0   \n","1   1        0    1    1     1         0   0         0         2        1   \n","2   1        0    1    0     0         0   1         1         1        1   \n","3   0        0    0    0     1         1   0         0         1        0   \n","\n","   natural  nlp  of  part  processing  subfields  tasks  \n","0        1    0   0     0           1          0      0  \n","1        0    0   1     0           0          1      0  \n","2        1    1   1     1           1          0      0  \n","3        0    1   0     0           0          0      1  \n"]}]},{"cell_type":"markdown","source":["**2. TF-IDF (Term Frequency-Inverse Document Frequency)**\n","Normalizes term frequencies by penalizing frequent words across documents.\n","Helps in giving more weight to unique terms in a document."],"metadata":{"id":"ArsERJyfYPtI"}},{"cell_type":"code","source":["# 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n","print(\"\\n--- TF-IDF ---\")\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","X_tfidf = tfidf_vectorizer.fit_transform(documents)\n","\n","# Convert to DataFrame\n","tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n","print(\"\\nTF-IDF Matrix:\\n\", tfidf_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFm6dR0bXaFm","executionInfo":{"status":"ok","timestamp":1738813595501,"user_tz":-330,"elapsed":19,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"36551410-0e88-47ed-b795-0cc70d7122e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- TF-IDF ---\n","\n","TF-IDF Matrix:\n","          ai   amazing       and       are      deep  improves        is  \\\n","0  0.000000  0.535566  0.000000  0.000000  0.000000  0.000000  0.422247   \n","1  0.303739  0.000000  0.303739  0.385254  0.303739  0.000000  0.000000   \n","2  0.297954  0.000000  0.297954  0.000000  0.000000  0.000000  0.297954   \n","3  0.000000  0.000000  0.000000  0.000000  0.412640  0.523381  0.000000   \n","\n","   language  learning   machine   natural       nlp        of      part  \\\n","0  0.422247  0.000000  0.000000  0.422247  0.000000  0.000000  0.000000   \n","1  0.000000  0.491805  0.303739  0.000000  0.000000  0.303739  0.000000   \n","2  0.297954  0.241220  0.297954  0.297954  0.297954  0.297954  0.377917   \n","3  0.000000  0.334067  0.000000  0.000000  0.412640  0.000000  0.000000   \n","\n","   processing  subfields     tasks  \n","0    0.422247   0.000000  0.000000  \n","1    0.000000   0.385254  0.000000  \n","2    0.297954   0.000000  0.000000  \n","3    0.000000   0.000000  0.523381  \n"]}]},{"cell_type":"code","source":["# Download the 'punkt_tab' data package\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgzsmtBXXsGj","executionInfo":{"status":"ok","timestamp":1738813655245,"user_tz":-330,"elapsed":411,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"b1efca46-fa4e-4de4-d600-0f6c4e10d84f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["**3. Word2Vec**\n","Creates word embeddings using the Continuous Bag of Words (CBOW) model.\n","Converts words into high-dimensional numeric vectors."],"metadata":{"id":"UiIJ-RE9YVLG"}},{"cell_type":"code","source":["# 3. Word2Vec Embeddings\n","print(\"\\n--- Word2Vec Embeddings ---\")\n","\n","# Tokenize text\n","tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n","\n","# Train Word2Vec model (using CBOW)\n","word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Get word embedding for a word\n","word = \"learning\"\n","if word in word2vec_model.wv:\n","    print(f\"\\nWord Embedding for '{word}':\\n\", word2vec_model.wv[word])\n","else:\n","    print(f\"\\nWord '{word}' not found in vocabulary.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJbYqADUXd3b","executionInfo":{"status":"ok","timestamp":1738813658577,"user_tz":-330,"elapsed":57,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"65d46fdd-8cf6-478d-e223-82813e7fcaf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Word2Vec Embeddings ---\n","\n","Word Embedding for 'learning':\n"," [-5.3808355e-04  2.4747057e-04  5.1054261e-03  9.0144686e-03\n"," -9.2937471e-03 -7.1226158e-03  6.4635528e-03  8.9830793e-03\n"," -5.0169979e-03 -3.7686056e-03  7.3833433e-03 -1.5425601e-03\n"," -4.5443131e-03  6.5564695e-03 -4.8607639e-03 -1.8169996e-03\n","  2.8797004e-03  9.9720282e-04 -8.2858307e-03 -9.4584674e-03\n","  7.3169568e-03  5.0672246e-03  6.7624357e-03  7.5818895e-04\n","  6.3456185e-03 -3.4061950e-03 -9.5028954e-04  5.7748272e-03\n"," -7.5254757e-03 -3.9375424e-03 -7.5109187e-03 -9.3785976e-04\n","  9.5394310e-03 -7.3277755e-03 -2.3322091e-03 -1.9385585e-03\n","  8.0853011e-03 -5.9210146e-03  4.6518860e-05 -4.7458964e-03\n"," -9.5945410e-03  4.9975729e-03 -8.7691769e-03 -4.3799556e-03\n"," -3.1460335e-05 -2.9525690e-04 -7.6617515e-03  9.6108336e-03\n","  4.9875192e-03  9.2362370e-03 -8.1496472e-03  4.4931308e-03\n"," -4.1252617e-03  8.2184700e-04  8.4948484e-03 -4.4612531e-03\n","  4.5260801e-03 -6.7854030e-03 -3.5504571e-03  9.4027435e-03\n"," -1.5833589e-03  3.1810731e-04 -4.1413419e-03 -7.6863044e-03\n"," -1.5081544e-03  2.4784398e-03 -8.8037999e-04  5.5386145e-03\n"," -2.7405014e-03  2.2650280e-03  5.4489318e-03  8.3431946e-03\n"," -1.4485705e-03 -9.2030661e-03  4.3761162e-03  5.7560828e-04\n","  7.4428865e-03 -8.0672430e-04 -2.6332915e-03 -8.7610492e-03\n"," -8.7329978e-04  2.8304288e-03  5.4048165e-03  7.0446525e-03\n"," -5.7038232e-03  1.8601780e-03  6.0962210e-03 -4.8040696e-03\n"," -3.1062963e-03  6.7972611e-03  1.6368956e-03  1.8905647e-04\n","  3.4769035e-03  2.1607307e-04  9.6252002e-03  5.0570723e-03\n"," -8.9138849e-03 -7.0379055e-03  8.9296431e-04  6.3993251e-03]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0vLJ7yZIXhoG"},"execution_count":null,"outputs":[]}]}